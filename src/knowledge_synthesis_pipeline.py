"""
çŸ¥è¯†åˆæˆæµæ°´çº¿ (Knowledge Synthesis Pipeline)
åè°ƒä¸‰é˜¶æ®µçš„æ™ºèƒ½åœ°è´¨æ•°æ®æå–å’ŒçŸ¥è¯†åˆæˆæµç¨‹
"""

import json
import logging
from typing import Dict, List, Optional, Any
from pathlib import Path
from datetime import datetime
import asyncio
from concurrent.futures import ThreadPoolExecutor

from config import load_config
from document_processing.pdf_processor import extract_full_text_from_pdf, chunk_text_by_paragraph
from entity_extraction.llm_extractor import extract_metadata, extract_tables, extract_knowledge_graph, configure_agent

# å¯¼å…¥æ–°çš„Agent
from agents.librarian_agent import LibrarianAgent
from agents.map_analyst_agent import MapAnalystAgent
from agents.geochemist_agent import GeochemistAgent
from agents.data_analyst_agent import DataAnalystAgent
from agents.synthesizer_agent import SynthesizerAgent

from models import Document


class KnowledgeSynthesisPipeline:
    """çŸ¥è¯†åˆæˆæµæ°´çº¿ä¸»ç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.agent_manager = configure_agent(
            agent_type=config["agent_config"]["agent_type"],
            agent_name=config["agent_config"]["agent_name"],
            api_key=config["google_api_key"]
        )
        
        # åˆå§‹åŒ–å„ä¸ªAgent
        self.librarian_agent = LibrarianAgent(agent_manager=self.agent_manager)
        self.map_analyst_agent = MapAnalystAgent(agent_manager=self.agent_manager)
        self.geochemist_agent = GeochemistAgent(agent_manager=self.agent_manager)
        self.data_analyst_agent = DataAnalystAgent(agent_manager=self.agent_manager)
        self.synthesizer_agent = SynthesizerAgent(agent_manager=self.agent_manager)
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        self.logger = logging.getLogger(__name__)
        
    def process_document(self, pdf_path: Path) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªPDFæ–‡æ¡£çš„å®Œæ•´æµæ°´çº¿
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            
        Returns:
            åŒ…å«æ‰€æœ‰å¤„ç†ç»“æœçš„å­—å…¸
        """
        self.logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£: {pdf_path.name}")
        
        try:
            # === é¢„å¤„ç†é˜¶æ®µ ===
            preprocessing_result = self._preprocess_document(pdf_path)
            if not preprocessing_result:
                return {'status': 'failed', 'stage': 'preprocessing'}
            
            # === ç¬¬ä¸€é˜¶æ®µï¼šæ™ºèƒ½åˆ†è¯Šä¸ä»»åŠ¡è§„åˆ’ ===
            self.logger.info("ğŸ“‹ é˜¶æ®µ 1: æ™ºèƒ½åˆ†è¯Šä¸ä»»åŠ¡è§„åˆ’")
            triage_result = self._stage1_intelligent_triage(preprocessing_result)
            
            # === ç¬¬äºŒé˜¶æ®µï¼šé¢†åŸŸä¸“å®¶æ·±åº¦åˆ†æ ===
            self.logger.info("ğŸ”¬ é˜¶æ®µ 2: é¢†åŸŸä¸“å®¶æ·±åº¦åˆ†æ")
            expert_analysis_result = self._stage2_expert_analysis(preprocessing_result, triage_result)
            
            # === ç¬¬ä¸‰é˜¶æ®µï¼šçŸ¥è¯†èåˆä¸æ•°æ®åº“æ˜ å°„ ===
            self.logger.info("ğŸ§  é˜¶æ®µ 3: çŸ¥è¯†èåˆä¸æ•°æ®åº“æ˜ å°„")
            synthesis_result = self._stage3_knowledge_synthesis(
                preprocessing_result, triage_result, expert_analysis_result
            )
            
            # === æ•´åˆæœ€ç»ˆç»“æœ ===
            final_result = self._compile_final_result(
                pdf_path, preprocessing_result, triage_result, 
                expert_analysis_result, synthesis_result
            )
            
            self.logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {pdf_path.name}")
            return final_result
            
        except Exception as e:
            self.logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'document': pdf_path.name
            }
    
    def _preprocess_document(self, pdf_path: Path) -> Optional[Dict[str, Any]]:
        """é¢„å¤„ç†æ–‡æ¡£"""
        try:
            self.logger.info("ğŸ“„ æå–PDFæ–‡æœ¬...")
            full_text = extract_full_text_from_pdf(pdf_path)
            
            if not full_text or len(full_text.strip()) < 100:
                self.logger.warning("PDFæ–‡æœ¬æå–å¤±è´¥æˆ–å†…å®¹è¿‡å°‘")
                return None
            
            self.logger.info("ğŸ“ åˆ†æ®µå¤„ç†æ–‡æœ¬...")
            chunks = chunk_text_by_paragraph(full_text)
            
            if not chunks:
                self.logger.warning("æ–‡æœ¬åˆ†æ®µå¤±è´¥")
                return None
            
            self.logger.info(f"âœ… é¢„å¤„ç†å®Œæˆ: {len(full_text)} å­—ç¬¦, {len(chunks)} æ®µè½")
            
            return {
                'pdf_path': str(pdf_path),
                'full_text': full_text,
                'chunks': chunks,
                'preprocessing_timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"é¢„å¤„ç†å¤±è´¥: {e}")
            return None
    
    def _stage1_intelligent_triage(self, preprocessing_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç¬¬ä¸€é˜¶æ®µï¼šæ™ºèƒ½åˆ†è¯Šä¸ä»»åŠ¡è§„åˆ’"""
        try:
            # è°ƒç”¨Librarian Agent
            librarian_input = {
                'pdf_path': preprocessing_result['pdf_path'],
                'full_text': preprocessing_result['full_text'],
                'chunks': preprocessing_result['chunks']
            }
            
            librarian_result = self.librarian_agent.process(librarian_input)
            
            self.logger.info(f"ğŸ“‹ ä»»åŠ¡è§„åˆ’å®Œæˆ: {len(librarian_result.get('task_plan', []))} ä¸ªä»»åŠ¡")
            
            return {
                'content_index': librarian_result.get('content_index'),
                'task_plan': librarian_result.get('task_plan', []),
                'status': librarian_result.get('status', 'unknown'),
                'stage1_timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"ç¬¬ä¸€é˜¶æ®µå¤±è´¥: {e}")
            return {'status': 'failed', 'error': str(e)}
    
    def _stage2_expert_analysis(self, preprocessing_result: Dict[str, Any], 
                              triage_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç¬¬äºŒé˜¶æ®µï¼šé¢†åŸŸä¸“å®¶æ·±åº¦åˆ†æ"""
        try:
            # é¦–å…ˆæå–åŸºç¡€è¡¨æ ¼æ•°æ®ï¼ˆç”¨äºåç»­åˆ†æï¼‰
            base_tables = self._extract_base_tables(preprocessing_result)
            
            # æ ¹æ®ä»»åŠ¡è®¡åˆ’å¹¶è¡Œæ‰§è¡Œä¸“å®¶åˆ†æ
            task_plan = triage_result.get('task_plan', [])
            content_index = triage_result.get('content_index')
            
            # å‡†å¤‡ä¸“å®¶åˆ†æçš„è¾“å…¥æ•°æ®
            analysis_input = {
                'content_units': content_index.content_units if content_index else [],
                'full_text': preprocessing_result['full_text'],
                'chunks': preprocessing_result['chunks'],
                'data_tables': base_tables
            }
            
            # å¹¶è¡Œæ‰§è¡Œä¸“å®¶åˆ†æ
            expert_results = self._run_expert_agents_parallel(analysis_input, task_plan)
            
            return {
                'map_analyst_result': expert_results.get('map_analyst', {}),
                'geochemist_result': expert_results.get('geochemist', {}),
                'data_analyst_result': expert_results.get('data_analyst', {}),
                'base_tables': base_tables,
                'stage2_timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"ç¬¬äºŒé˜¶æ®µå¤±è´¥: {e}")
            return {'status': 'failed', 'error': str(e)}
    
    def _extract_base_tables(self, preprocessing_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æå–åŸºç¡€è¡¨æ ¼æ•°æ®"""
        try:
            self.logger.info("ğŸ“Š æå–åŸºç¡€è¡¨æ ¼æ•°æ®...")
            tables = extract_tables(self.agent_manager, preprocessing_result['full_text'])
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            base_tables = []
            for table in tables:
                if hasattr(table, 'model_dump'):
                    base_tables.append(table.model_dump())
                elif isinstance(table, dict):
                    base_tables.append(table)
            
            self.logger.info(f"ğŸ“Š æå–åˆ° {len(base_tables)} ä¸ªåŸºç¡€è¡¨æ ¼")
            return base_tables
            
        except Exception as e:
            self.logger.warning(f"åŸºç¡€è¡¨æ ¼æå–å¤±è´¥: {e}")
            return []
    
    def _run_expert_agents_parallel(self, analysis_input: Dict[str, Any], 
                                  task_plan: List[Any]) -> Dict[str, Any]:
        """å¹¶è¡Œè¿è¡Œä¸“å®¶Agent"""
        expert_results = {}
        
        # ç¡®å®šéœ€è¦è¿è¡Œçš„ä¸“å®¶Agent
        agents_to_run = set()
        for task in task_plan:
            if hasattr(task, 'agent_type'):
                agents_to_run.add(task.agent_type)
            elif isinstance(task, dict):
                agents_to_run.add(task.get('agent_type', ''))
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = {}
            
            if 'map_analyst' in agents_to_run:
                futures['map_analyst'] = executor.submit(
                    self.map_analyst_agent.process, analysis_input
                )
            
            if 'geochemist' in agents_to_run:
                futures['geochemist'] = executor.submit(
                    self.geochemist_agent.process, analysis_input
                )
            
            if 'data_analyst' in agents_to_run:
                # Data Analystéœ€è¦åŸå§‹è¡¨æ ¼æ•°æ®
                data_input = {**analysis_input, 'raw_tables': analysis_input['data_tables']}
                futures['data_analyst'] = executor.submit(
                    self.data_analyst_agent.process, data_input
                )
            
            # æ”¶é›†ç»“æœ
            for agent_name, future in futures.items():
                try:
                    result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                    expert_results[agent_name] = result
                    self.logger.info(f"âœ… {agent_name} åˆ†æå®Œæˆ")
                except Exception as e:
                    self.logger.error(f"âŒ {agent_name} åˆ†æå¤±è´¥: {e}")
                    expert_results[agent_name] = {'status': 'failed', 'error': str(e)}
        
        return expert_results
    
    def _stage3_knowledge_synthesis(self, preprocessing_result: Dict[str, Any],
                                  triage_result: Dict[str, Any], 
                                  expert_analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç¬¬ä¸‰é˜¶æ®µï¼šçŸ¥è¯†èåˆä¸æ•°æ®åº“æ˜ å°„"""
        try:
            # æå–åŸå§‹çŸ¥è¯†å›¾è°±ï¼ˆå¦‚æœéœ€è¦ï¼‰
            original_kg = self._extract_original_knowledge_graph(preprocessing_result)
            
            # å‡†å¤‡Synthesizerè¾“å…¥
            synthesis_input = {
                'document_source': Path(preprocessing_result['pdf_path']).name,
                'map_analyst_result': expert_analysis_result.get('map_analyst_result', {}),
                'geochemist_result': expert_analysis_result.get('geochemist_result', {}),
                'data_analyst_result': expert_analysis_result.get('data_analyst_result', {}),
                'original_knowledge_graph': original_kg
            }
            
            # è°ƒç”¨Synthesizer Agent
            synthesis_result = self.synthesizer_agent.process(synthesis_input)
            
            self.logger.info(f"ğŸ§  çŸ¥è¯†åˆæˆå®Œæˆ: {synthesis_result.get('database_records_count', 0)} æ¡æ•°æ®åº“è®°å½•")
            
            return {
                'synthesized_knowledge': synthesis_result.get('synthesized_knowledge'),
                'database_records_count': synthesis_result.get('database_records_count', 0),
                'overall_confidence': synthesis_result.get('overall_confidence', 0.0),
                'processing_status': synthesis_result.get('processing_status', 'unknown'),
                'stage3_timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"ç¬¬ä¸‰é˜¶æ®µå¤±è´¥: {e}")
            return {'status': 'failed', 'error': str(e)}
    
    def _extract_original_knowledge_graph(self, preprocessing_result: Dict[str, Any]) -> Dict[str, Any]:
        """æå–åŸå§‹çŸ¥è¯†å›¾è°±"""
        try:
            # ä½¿ç”¨å‰å‡ ä¸ªchunksæå–çŸ¥è¯†å›¾è°±
            kg_text = " ".join(preprocessing_result['chunks'][:5])
            kg = extract_knowledge_graph(self.agent_manager, kg_text)
            
            if kg and hasattr(kg, 'model_dump'):
                return kg.model_dump()
            elif isinstance(kg, dict):
                return kg
            else:
                return {}
                
        except Exception as e:
            self.logger.warning(f"åŸå§‹çŸ¥è¯†å›¾è°±æå–å¤±è´¥: {e}")
            return {}
    
    def _compile_final_result(self, pdf_path: Path, preprocessing_result: Dict[str, Any],
                            triage_result: Dict[str, Any], expert_analysis_result: Dict[str, Any],
                            synthesis_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç¼–è¯‘æœ€ç»ˆç»“æœ"""
        
        # åˆ›å»ºä¼ ç»ŸDocumentå¯¹è±¡ï¼ˆå‘åå…¼å®¹ï¼‰
        try:
            # æå–åŸºç¡€å…ƒæ•°æ®
            metadata = extract_metadata(self.agent_manager, preprocessing_result['chunks'][0])
            
            document = Document(
                metadata=metadata,
                extracted_tables=expert_analysis_result.get('base_tables', []),
                knowledge_graph=synthesis_result.get('synthesized_knowledge', {}).get('knowledge_graph', {}),
                source_file=pdf_path.name,
                processing_timestamp_utc=datetime.now().isoformat(),
                full_text=preprocessing_result['full_text'][:1000] + "..."
            )
        except Exception as e:
            self.logger.warning(f"åˆ›å»ºDocumentå¯¹è±¡å¤±è´¥: {e}")
            document = None
        
        # ç¼–è¯‘å®Œæ•´ç»“æœ
        final_result = {
            'status': 'success',
            'document': document,
            'pipeline_results': {
                'stage1_triage': triage_result,
                'stage2_expert_analysis': expert_analysis_result,
                'stage3_synthesis': synthesis_result
            },
            'summary': {
                'document_name': pdf_path.name,
                'processing_timestamp': datetime.now().isoformat(),
                'total_tasks_planned': len(triage_result.get('task_plan', [])),
                'spatial_features_count': expert_analysis_result.get('map_analyst_result', {}).get('feature_count', 0),
                'geochemical_conclusions_count': expert_analysis_result.get('geochemist_result', {}).get('conclusions_count', 0),
                'standardized_tables_count': expert_analysis_result.get('data_analyst_result', {}).get('standardized_tables_count', 0),
                'database_records_count': synthesis_result.get('database_records_count', 0),
                'overall_confidence': synthesis_result.get('overall_confidence', 0.0)
            }
        }
        
        return final_result
    
    def save_results(self, final_result: Dict[str, Any], output_dir: Path) -> Dict[str, str]:
        """ä¿å­˜å¤„ç†ç»“æœ"""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        document_name = final_result['summary']['document_name']
        base_name = Path(document_name).stem
        
        saved_files = {}
        
        try:
            # 1. ä¿å­˜å®Œæ•´çš„æµæ°´çº¿ç»“æœ
            pipeline_file = output_dir / f"{base_name}_pipeline_result.json"
            with open(pipeline_file, 'w', encoding='utf-8') as f:
                # ä½¿ç”¨è‡ªå®šä¹‰ç¼–ç å™¨å¤„ç†å¤æ‚å¯¹è±¡
                json.dump(final_result, f, indent=2, default=self._json_serializer, ensure_ascii=False)
            saved_files['pipeline_result'] = str(pipeline_file)
            
            # 2. ä¿å­˜æ•°æ®åº“è®°å½•ï¼ˆå¦‚æœæœ‰ï¼‰
            synthesis_result = final_result.get('pipeline_results', {}).get('stage3_synthesis', {})
            synthesized_knowledge = synthesis_result.get('synthesized_knowledge')
            
            if synthesized_knowledge and hasattr(synthesized_knowledge, 'database_records'):
                db_records_file = output_dir / f"{base_name}_database_records.json"
                records_data = [
                    record.model_dump() if hasattr(record, 'model_dump') else record
                    for record in synthesized_knowledge.database_records
                ]
                with open(db_records_file, 'w', encoding='utf-8') as f:
                    json.dump(records_data, f, indent=2, ensure_ascii=False)
                saved_files['database_records'] = str(db_records_file)
            
            # 3. ä¿å­˜ä¼ ç»Ÿæ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
            if final_result.get('document'):
                traditional_file = output_dir / f"{base_name}_traditional.json"
                with open(traditional_file, 'w', encoding='utf-8') as f:
                    document_data = final_result['document']
                    if hasattr(document_data, 'model_dump_json'):
                        f.write(document_data.model_dump_json(indent=2))
                    else:
                        json.dump(document_data, f, indent=2, default=self._json_serializer, ensure_ascii=False)
                saved_files['traditional_format'] = str(traditional_file)
            
            self.logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            saved_files['error'] = str(e)
        
        return saved_files
    
    def _json_serializer(self, obj):
        """JSONåºåˆ—åŒ–å™¨"""
        if hasattr(obj, 'model_dump'):
            return obj.model_dump()
        elif hasattr(obj, '__dict__'):
            return obj.__dict__
        else:
            return str(obj)
