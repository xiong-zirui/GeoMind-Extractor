#!/usr/bin/env python3
"""
æ™ºèƒ½åœ°è´¨æ•°æ®æå–ç³»ç»Ÿ - ä¸»å…¥å£ç¨‹åº
ä½¿ç”¨ä¸‰é˜¶æ®µçŸ¥è¯†åˆæˆæµæ°´çº¿å¤„ç†åœ°è´¨æ–‡çŒ®
"""

import argparse
import logging
from pathlib import Path
import json
from datetime import datetime

from config import load_config
from knowledge_synthesis_pipeline import KnowledgeSynthesisPipeline


def setup_logging(log_level: str = "INFO") -> None:
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    log_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format=log_format,
        datefmt="%Y-%m-%d %H:%M:%S"
    )


def process_single_document(pdf_path: Path, config: dict, output_dir: Path) -> dict:
    """å¤„ç†å•ä¸ªPDFæ–‡æ¡£"""
    logger = logging.getLogger(__name__)
    
    logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£: {pdf_path.name}")
    
    # åˆå§‹åŒ–æµæ°´çº¿
    pipeline = KnowledgeSynthesisPipeline(config)
    
    # å¤„ç†æ–‡æ¡£
    result = pipeline.process_document(pdf_path)
    
    if result.get('status') == 'success':
        # ä¿å­˜ç»“æœ
        saved_files = pipeline.save_results(result, output_dir)
        
        # æ‰“å°å¤„ç†æ‘˜è¦
        summary = result.get('summary', {})
        print(f"\nâœ… æ–‡æ¡£ '{pdf_path.name}' å¤„ç†å®Œæˆ!")
        print(f"ğŸ“Š å¤„ç†æ‘˜è¦:")
        print(f"   â€¢ è®¡åˆ’ä»»åŠ¡æ•°: {summary.get('total_tasks_planned', 0)}")
        print(f"   â€¢ ç©ºé—´è¦ç´ æ•°: {summary.get('spatial_features_count', 0)}")
        print(f"   â€¢ åœ°çƒåŒ–å­¦ç»“è®ºæ•°: {summary.get('geochemical_conclusions_count', 0)}")
        print(f"   â€¢ æ ‡å‡†åŒ–è¡¨æ ¼æ•°: {summary.get('standardized_tables_count', 0)}")
        print(f"   â€¢ æ•°æ®åº“è®°å½•æ•°: {summary.get('database_records_count', 0)}")
        print(f"   â€¢ æ€»ä½“ç½®ä¿¡åº¦: {summary.get('overall_confidence', 0.0):.2%}")
        
        print(f"ğŸ’¾ ç»“æœæ–‡ä»¶:")
        for file_type, file_path in saved_files.items():
            print(f"   â€¢ {file_type}: {file_path}")
        
        return {'status': 'success', 'saved_files': saved_files, 'summary': summary}
    
    else:
        error_msg = result.get('error', 'Unknown error')
        logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {error_msg}")
        return {'status': 'failed', 'error': error_msg}


def process_batch_documents(input_dir: Path, config: dict, output_dir: Path) -> dict:
    """æ‰¹é‡å¤„ç†PDFæ–‡æ¡£"""
    logger = logging.getLogger(__name__)
    
    # æŸ¥æ‰¾æ‰€æœ‰PDFæ–‡ä»¶
    pdf_files = list(input_dir.glob("*.pdf"))
    
    if not pdf_files:
        logger.warning(f"åœ¨ç›®å½• {input_dir} ä¸­æœªæ‰¾åˆ°PDFæ–‡ä»¶")
        return {'status': 'no_files', 'processed': 0, 'failed': 0}
    
    logger.info(f"æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶ï¼Œå¼€å§‹æ‰¹é‡å¤„ç†...")
    
    # æ‰¹é‡å¤„ç†ç»Ÿè®¡
    processed_count = 0
    failed_count = 0
    results = []
    
    for pdf_path in pdf_files:
        try:
            result = process_single_document(pdf_path, config, output_dir)
            results.append({
                'file': pdf_path.name,
                'result': result
            })
            
            if result['status'] == 'success':
                processed_count += 1
            else:
                failed_count += 1
                
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶ {pdf_path.name} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            failed_count += 1
            results.append({
                'file': pdf_path.name,
                'result': {'status': 'exception', 'error': str(e)}
            })
    
    # ä¿å­˜æ‰¹é‡å¤„ç†æŠ¥å‘Š
    batch_report = {
        'processing_timestamp': datetime.now().isoformat(),
        'total_files': len(pdf_files),
        'processed_successfully': processed_count,
        'failed': failed_count,
        'success_rate': processed_count / len(pdf_files) if pdf_files else 0,
        'detailed_results': results
    }
    
    report_file = output_dir / "batch_processing_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(batch_report, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“ˆ æ‰¹é‡å¤„ç†å®Œæˆ!")
    print(f"   â€¢ æ€»æ–‡ä»¶æ•°: {len(pdf_files)}")
    print(f"   â€¢ æˆåŠŸå¤„ç†: {processed_count}")
    print(f"   â€¢ å¤„ç†å¤±è´¥: {failed_count}")
    print(f"   â€¢ æˆåŠŸç‡: {batch_report['success_rate']:.1%}")
    print(f"   â€¢ è¯¦ç»†æŠ¥å‘Š: {report_file}")
    
    return batch_report


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="æ™ºèƒ½åœ°è´¨æ•°æ®æå–ç³»ç»Ÿ - ä¸‰é˜¶æ®µçŸ¥è¯†åˆæˆæµæ°´çº¿",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å¤„ç†å•ä¸ªPDFæ–‡ä»¶
  python main_intelligent.py --input data/raw/theses-WAXI/2008_MATABANE_FE3.pdf
  
  # æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰PDFæ–‡ä»¶
  python main_intelligent.py --input data/raw/theses-WAXI/ --batch
  
  # æŒ‡å®šè¾“å‡ºç›®å½•å’Œæ—¥å¿—çº§åˆ«
  python main_intelligent.py --input file.pdf --output results/ --log-level DEBUG
        """
    )
    
    parser.add_argument(
        "--input", "-i",
        type=str,
        required=True,
        help="è¾“å…¥PDFæ–‡ä»¶è·¯å¾„æˆ–åŒ…å«PDFæ–‡ä»¶çš„ç›®å½•è·¯å¾„"
    )
    
    parser.add_argument(
        "--output", "-o",
        type=str,
        default="data/processed/",
        help="è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: data/processed/)"
    )
    
    parser.add_argument(
        "--batch", "-b",
        action="store_true",
        help="æ‰¹é‡å¤„ç†æ¨¡å¼ï¼šå¤„ç†è¾“å…¥ç›®å½•ä¸­çš„æ‰€æœ‰PDFæ–‡ä»¶"
    )
    
    parser.add_argument(
        "--log-level", "-l",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)"
    )
    
    parser.add_argument(
        "--config", "-c",
        type=str,
        help="é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config.yml)"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(args.log_level)
    logger = logging.getLogger(__name__)
    
    # åŠ è½½é…ç½®
    try:
        if args.config:
            config = load_config(args.config)
        else:
            config = load_config()
    except Exception as e:
        logger.error(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
        return 1
    
    # éªŒè¯è¾“å…¥è·¯å¾„
    input_path = Path(args.input)
    if not input_path.exists():
        logger.error(f"è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_path}")
        return 1
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # å¤„ç†é€»è¾‘
    try:
        if args.batch:
            # æ‰¹é‡å¤„ç†æ¨¡å¼
            if not input_path.is_dir():
                logger.error("æ‰¹é‡å¤„ç†æ¨¡å¼éœ€è¦è¾“å…¥ç›®å½•è·¯å¾„")
                return 1
            
            result = process_batch_documents(input_path, config, output_dir)
            
        else:
            # å•æ–‡ä»¶å¤„ç†æ¨¡å¼
            if input_path.is_dir():
                logger.error("å•æ–‡ä»¶å¤„ç†æ¨¡å¼éœ€è¦è¾“å…¥PDFæ–‡ä»¶è·¯å¾„")
                return 1
            
            if not input_path.suffix.lower() == '.pdf':
                logger.error("è¾“å…¥æ–‡ä»¶å¿…é¡»æ˜¯PDFæ ¼å¼")
                return 1
            
            result = process_single_document(input_path, config, output_dir)
        
        # æ ¹æ®å¤„ç†ç»“æœè¿”å›é€‚å½“çš„é€€å‡ºç 
        if result['status'] == 'success':
            return 0
        elif result['status'] == 'no_files':
            return 0  # æ²¡æœ‰æ–‡ä»¶å¯å¤„ç†ä¸ç®—é”™è¯¯
        else:
            return 1
            
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­å¤„ç†")
        return 130
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
        return 1


if __name__ == "__main__":
    exit(main())
