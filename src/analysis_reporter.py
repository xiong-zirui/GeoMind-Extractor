import json
from pathlib import Path
from datetime import datetime
import webbrowser

def generate_analysis_report(processed_dir):
    """Generate a comprehensive analysis report"""
    
    # Find the latest analysis files
    analysis_files = list(Path(processed_dir).glob("*_complete_analysis.json"))
    if not analysis_files:
        print("âŒ No analysis files found")
        return
    
    latest_file = max(analysis_files, key=lambda x: x.stat().st_mtime)
    print(f"ðŸ“‹ Reading analysis from: {latest_file.name}")
    
    with open(latest_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # Extract information
    doc_info = data['document_info']
    categories = data['categorized_entities']
    relationships = data['relationship_analysis']
    insights = data['insights']
    raw_data = data['raw_extraction']
    
    print("\n" + "="*80)
    print("ðŸ“Š GEODATA EXTRACTION & ANALYSIS REPORT")
    print("="*80)
    
    # Document Information
    print(f"\nðŸ“„ Document: {doc_info['filename']}")
    print(f"â° Processed: {doc_info['processing_time']}")
    print(f"ðŸŽ¯ Confidence Scores:")
    print(f"   â€¢ Metadata: {doc_info['confidence_scores']['metadata']*100:.1f}%")
    print(f"   â€¢ Knowledge Graph: {doc_info['confidence_scores']['knowledge_graph']*100:.1f}%")
    
    # Metadata Summary
    metadata = raw_data['metadata']
    print(f"\nðŸ“‘ Document Metadata:")
    print(f"   â€¢ Title: {metadata['title']}")
    print(f"   â€¢ Author(s): {', '.join(metadata['authors'])}")
    print(f"   â€¢ Year: {metadata['publication_year']}")
    print(f"   â€¢ Keywords: {', '.join(metadata['keywords'])}")
    
    # Categories Analysis
    print(f"\nðŸ·ï¸ ENTITY CATEGORIZATION:")
    total_entities = sum(len(entities) for entities in categories.values() if entities)
    print(f"   ðŸ“Š Total Categorized Entities: {total_entities}")
    
    for category, entities in categories.items():
        if entities:
            category_name = category.replace('_', ' ').title()
            print(f"\n   ðŸ“ {category_name} ({len(entities)} entities):")
            for entity in entities:
                print(f"      â€¢ {entity['name']} ({entity['type']})")
    
    # Relationships Analysis
    print(f"\nðŸ”— RELATIONSHIP ANALYSIS:")
    total_relationships = sum(len(rels) for rels in relationships.values() if rels)
    print(f"   ðŸ“Š Total Relationships: {total_relationships}")
    
    for rel_type, rels in relationships.items():
        if rels:
            type_name = rel_type.replace('_', ' ').title()
            print(f"\n   ðŸ”— {type_name} ({len(rels)} relationships):")
            for rel in rels[:5]:  # Show first 5
                print(f"      â€¢ {rel['source']} â†’ {rel['relationship']} â†’ {rel['target']}")
            if len(rels) > 5:
                print(f"      ... and {len(rels) - 5} more")
    
    # Key Insights
    print(f"\nðŸ’¡ KEY INSIGHTS:")
    for finding in insights['key_findings']:
        print(f"   â€¢ {finding}")
    
    if insights['geological_context']:
        print(f"\nðŸ—ºï¸ GEOLOGICAL CONTEXT:")
        for key, value in insights['geological_context'].items():
            context_name = key.replace('_', ' ').title()
            if isinstance(value, list):
                print(f"   â€¢ {context_name}: {', '.join(value)}")
            else:
                print(f"   â€¢ {context_name}: {value}")
    
    print(f"\nðŸ“‹ RECOMMENDATIONS:")
    for rec in insights['recommendations']:
        print(f"   â€¢ {rec}")
    
    # Processing Statistics
    print(f"\nðŸ“ˆ PROCESSING STATISTICS:")
    summary = insights['summary']
    print(f"   â€¢ Total Entities Extracted: {summary['total_entities']}")
    print(f"   â€¢ Total Relationships: {summary['total_relationships']}")
    print(f"   â€¢ Categories with Data: {len([k for k, v in summary['categories_found'].items() if v > 0])}")
    print(f"   â€¢ Category Distribution:")
    for category, count in summary['categories_found'].items():
        category_name = category.replace('_', ' ').title()
        print(f"     - {category_name}: {count}")
    
    # Data Quality Assessment
    print(f"\nðŸŽ¯ DATA QUALITY ASSESSMENT:")
    
    # Check extraction completeness
    has_metadata = bool(metadata['title'] and metadata['authors'])
    has_entities = summary['total_entities'] > 0
    has_relationships = summary['total_relationships'] > 0
    has_categories = len(summary['categories_found']) > 0
    
    quality_score = sum([has_metadata, has_entities, has_relationships, has_categories]) / 4 * 100
    
    print(f"   â€¢ Overall Quality Score: {quality_score:.1f}%")
    print(f"   â€¢ Metadata Extraction: {'âœ… Success' if has_metadata else 'âŒ Failed'}")
    print(f"   â€¢ Entity Extraction: {'âœ… Success' if has_entities else 'âŒ Failed'}")
    print(f"   â€¢ Relationship Extraction: {'âœ… Success' if has_relationships else 'âŒ Failed'}")
    print(f"   â€¢ Categorization: {'âœ… Success' if has_categories else 'âŒ Failed'}")
    
    # Identify areas for improvement
    print(f"\nðŸ”§ AREAS FOR IMPROVEMENT:")
    
    if not raw_data['extracted_tables']:
        print(f"   â€¢ Table extraction failed - needs improvement in structured data parsing")
    
    empty_categories = [k.replace('_', ' ').title() for k, v in categories.items() if not v]
    if empty_categories:
        print(f"   â€¢ Missing categories: {', '.join(empty_categories[:3])}")
        if len(empty_categories) > 3:
            print(f"     and {len(empty_categories) - 3} more...")
    
    if summary['total_relationships'] < summary['total_entities']:
        print(f"   â€¢ Low relationship density - could extract more entity connections")
    
    print(f"\n" + "="*80)
    print("âœ¨ ANALYSIS COMPLETE")
    print("="*80)
    
    # Generate a simple text report
    report_file = Path(processed_dir) / f"{latest_file.stem}_report.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("GEODATA EXTRACTION & ANALYSIS REPORT\n")
        f.write("="*50 + "\n\n")
        f.write(f"Document: {doc_info['filename']}\n")
        f.write(f"Processed: {doc_info['processing_time']}\n\n")
        
        f.write("SUMMARY:\n")
        f.write(f"- Total Entities: {summary['total_entities']}\n")
        f.write(f"- Total Relationships: {summary['total_relationships']}\n")
        f.write(f"- Quality Score: {quality_score:.1f}%\n\n")
        
        f.write("CATEGORIES:\n")
        for category, entities in categories.items():
            if entities:
                f.write(f"- {category.replace('_', ' ').title()}: {len(entities)} entities\n")
        
        f.write("\nKEY FINDINGS:\n")
        for finding in insights['key_findings']:
            f.write(f"- {finding}\n")
    
    print(f"ðŸ“„ Text report saved to: {report_file}")

if __name__ == "__main__":
    processed_dir = "data/processed"
    generate_analysis_report(processed_dir)
